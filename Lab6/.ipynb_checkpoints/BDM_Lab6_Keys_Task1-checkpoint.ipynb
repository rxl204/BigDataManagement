{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172-16-37-45.dynapool.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're creating two RDD, one is from the README file of Spark\n",
    "# and the other is directly from a list within the notebook.\n",
    "#\n",
    "# If you downloaded Spark, the README file is in the same folder\n",
    "# as the one you extracted. If you use other package management\n",
    "# methods like 'brew', 'dnf', 'apt', etc. you will need to figure\n",
    "# out the path of Spark by printing sys.path.\n",
    "\n",
    "rdd = sc.textFile('/Users/rachellim/CUSP/BDM/Lab6/README.md')\n",
    "testRdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'#CDS_DataViz']]\n",
      "\n",
      "[u'#CDS_DataViz']\n"
     ]
    }
   ],
   "source": [
    "# Here, we're showing the difference between map() and flatMap()\n",
    "# doing the line split and get back the first 3 elements, take(3).\n",
    "# - map() is a one-to-one mapping, just like Python, so the first\n",
    "# line prints out 3 lists, each consists words per each.\n",
    "# - flatMap() is a one-to-many mapping, like MapReduce's map(). So\n",
    "# the second line prints out only 3 words.\n",
    "\n",
    "wordsPerLine = rdd.map(lambda line: line.split()).take(3)\n",
    "words = rdd.flatMap(lambda line: line.split()).take(3)\n",
    "\n",
    "print ('%s\\n\\n%s' % (wordsPerLine, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#cds_dataviz', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the word count example with Spark using the approach\n",
    "# shown in the slides, i.e. staying true to the MapReduce paradigm.\n",
    "# Note that groupByKey() will sort and group everything together by\n",
    "# keys first. Then the function in mapValues() will each get applied\n",
    "# per each (key, list of values) pair. This could be an issue if we\n",
    "# have a pair with lots of values since all of the values have to be\n",
    "# stored in memory.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda values: sum(values))\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#cds_dataviz', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is another approach with reduceByKey() instead of groupByKey().\n",
    "# The reduce function provided for reduceByKey() only takes 2 params\n",
    "# at a time, thus, doesn't suffer the scalability issue. It also has\n",
    "# better benefits in term of parallelism.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#cds_dataviz', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we'd like to compute the top 3 most popular words in Spark. We\n",
    "# can use the RDD's top() function directly. This is much easier\n",
    "# than the two-step MapReduce job, where we had to first compute the\n",
    "# top 3 words per partition, then another top 3 on top of that. In\n",
    "# fact, this is exactly how Spark RDD's top() function is implemented.\n",
    "# More info can be found here:\n",
    "# https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L1249\n",
    "wc.top(3, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 6 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAT_FN = 'SAT_Results.csv'\n",
    "HSD_FN = 'DOE_High_School_Directory_2014-2015.csv'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Below is a way to read CSV file from within Spark directly into a \n",
    "# Spark's DataFrame, which we will not be covering yet. Just putting\n",
    "# it here so that we have a reference for now. Note that, the \n",
    "# 'parserLib' option is important for reading multi-line fields of CSV.\n",
    "df = spark.read \\\n",
    "            .format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"parserLib\", \"UNIVOCITY\") \\\n",
    "            .load(HSD_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'DBN'),\n",
       " (1, 'SCHOOL NAME'),\n",
       " (2, 'Num of SAT Test Takers'),\n",
       " (3, 'SAT Critical Reading Avg. Score'),\n",
       " (4, 'SAT Math Avg. Score'),\n",
       " (5, 'SAT Writing Avg. Score')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read the SAT score to our RDD. Note that the use_unicode can be\n",
    "# changed accordingly to your data file to handle Unicode. If you cannot\n",
    "# parse your data due to an 'utf8' or 'ascii' decoding issue, it might\n",
    "# be a good thing to try flipping the use_unicode parameter here.\n",
    "\n",
    "sat = sc.textFile(SAT_FN, use_unicode=False).cache()\n",
    "\n",
    "# This line for us to list the column index and column names to see\n",
    "# which column we need to use for our task. In this case, we're\n",
    "# interested in the number of test takers (#2) and the math score (#4).\n",
    "list(enumerate(sat.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN,SCHOOL NAME,Num of SAT Test Takers,SAT Critical Reading Avg. Score,SAT Math Avg. Score,SAT Writing Avg. Score\n",
      "02M047,47 THE AMERICAN SIGN LANGUAGE AND ENGLISH SECONDARY SCHOOL,16,395,400,387\n"
     ]
    }
   ],
   "source": [
    "# Note that, our data input includes a header line that we don't want to\n",
    "# use in analysis. We can remove the header line from our RDD by doing\n",
    "# a 'filter' to remove all rows that matches the header like below. Though\n",
    "# this works, it means that we have to apply the filter function on *all*\n",
    "# row, which could be a lot of computation.\n",
    "\n",
    "noHeaderRDD = sat.filter(lambda x: not x.startswith('DBN,SCHOOL'))\n",
    "print (sat.first())\n",
    "print (noHeaderRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('02M047', (6400, 16)),\n",
       " ('21K410', (207575, 475)),\n",
       " ('30Q301', (43120, 98)),\n",
       " ('17K382', (22066, 59)),\n",
       " ('18K637', (13335, 35))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can perform the header checking per-partition, instead\n",
    "# of per-row like below. mapPartitions() is another type of map operators\n",
    "# in Spark that is similar to Hadoop Streaming's map(). It is many-to-many.\n",
    "# RDD in Spark are divided into partitions (as we read or as provided by\n",
    "# HDFS), each partition can be processed in parallel using a function\n",
    "# supplied to the mapPartitions() call.\n",
    "# \n",
    "# In addition to mapPartitions(), Spark also provides a variation called\n",
    "# mapPartitionsWithIndex() that provides information on which partition\n",
    "# we are currently processing. Indeed, mapPartitionsWithIndex() is the\n",
    "# the operator with the lowest overhead (since mapPartitions() get mapped\n",
    "# to mapPartitionsWithIndex) and also the most efficient one among all the\n",
    "# map operators.\n",
    "#\n",
    "# So our logic below is to use the partition index to check if we're hitting\n",
    "# the header (aka the first partition). If so, we just skip the first row.\n",
    "\n",
    "def extractScores(partId, records):\n",
    "    if partId==0:\n",
    "        records.next()\n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for row in reader:\n",
    "        if row[2]!='s': # to filter our bad-quality data\n",
    "            (dbn,takers,score) = (row[0], int(row[2]), int(row[4]))\n",
    "            yield (dbn, (score*takers, takers))\n",
    "\n",
    "satScores = sat.mapPartitionsWithIndex(extractScores)\n",
    "satScores.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'dbn'),\n",
       " (1, 'school_name'),\n",
       " (2, 'boro'),\n",
       " (3, 'building_code'),\n",
       " (4, 'phone_number'),\n",
       " (5, 'fax_number'),\n",
       " (6, 'grade_span_min'),\n",
       " (7, 'grade_span_max'),\n",
       " (8, 'expgrade_span_min'),\n",
       " (9, 'expgrade_span_max'),\n",
       " (10, 'bus'),\n",
       " (11, 'subway'),\n",
       " (12, 'primary_address_line_1'),\n",
       " (13, 'city'),\n",
       " (14, 'state_code'),\n",
       " (15, 'zip'),\n",
       " (16, 'website'),\n",
       " (17, 'total_students'),\n",
       " (18, 'campus_name'),\n",
       " (19, 'school_type'),\n",
       " (20, 'overview_paragraph'),\n",
       " (21, 'program_highlights'),\n",
       " (22, 'language_classes'),\n",
       " (23, 'advancedplacement_courses'),\n",
       " (24, 'online_ap_courses'),\n",
       " (25, 'online_language_courses'),\n",
       " (26, 'extracurricular_activities'),\n",
       " (27, 'psal_sports_boys'),\n",
       " (28, 'psal_sports_girls'),\n",
       " (29, 'psal_sports_coed'),\n",
       " (30, 'school_sports'),\n",
       " (31, 'partner_cbo'),\n",
       " (32, 'partner_hospital'),\n",
       " (33, 'partner_highered'),\n",
       " (34, 'partner_cultural'),\n",
       " (35, 'partner_nonprofit'),\n",
       " (36, 'partner_corporate'),\n",
       " (37, 'partner_financial'),\n",
       " (38, 'partner_other'),\n",
       " (39, 'addtl_info1'),\n",
       " (40, 'addtl_info2'),\n",
       " (41, 'start_time'),\n",
       " (42, 'end_time'),\n",
       " (43, 'se_services'),\n",
       " (44, 'ell_programs'),\n",
       " (45, 'school_accessibility_description'),\n",
       " (46, 'number_programs'),\n",
       " (47, 'priority01'),\n",
       " (48, 'priority02'),\n",
       " (49, 'priority03'),\n",
       " (50, 'priority04'),\n",
       " (51, 'priority05'),\n",
       " (52, 'priority06'),\n",
       " (53, 'priority07'),\n",
       " (54, 'priority08'),\n",
       " (55, 'priority09'),\n",
       " (56, 'priority10'),\n",
       " (57, 'Location 1')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the same thing with the school directory data\n",
    "schools = sc.textFile(HSD_FN, use_unicode=False).cache()\n",
    "list(enumerate(schools.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractSchools(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        list_of_records.next() # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58 and row[17].isdigit():\n",
    "            (dbn, boro, total_students) = (row[0], row[2], int(row[17]))\n",
    "            if total_students>500: # filter to keep the large schools\n",
    "                yield (dbn, boro)\n",
    "\n",
    "largeSchools = schools.mapPartitionsWithIndex(extractSchools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bronx', 470),\n",
       " ('Manhattan', 514),\n",
       " ('Brooklyn', 487),\n",
       " ('Staten Island', 477),\n",
       " ('Queens', 474)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buses = \"B39, M14A, M14D, M15, M15-SBS, M21, M9\"\n",
    "subways = \"B, D to Grand St; F to East Broadway ; J, M, Z to Delancey St-Essex St\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'M', 'Z']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = buses.split(', ')\n",
    "lines = subways.split(' ; ')[1].split(' to ')[0].split(', ')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowLen = len(schools.first().split(','))\n",
    "\n",
    "def extractSchools2(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==rowLen:\n",
    "            (dbn, buses, subways) = (row[0], row[10], row[11])\n",
    "            lines = buses.split(', ') + ', '.join([direction.split(' to' )[0]\n",
    "                                                  for direction in subways.split(' ; ')]).split(', ') \n",
    "    yield (dbn, lines)\n",
    "\n",
    "schoolsWithLines = schools.mapPartitionsWithIndex(extractSchools2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('32K556',\n",
       "  (['B38', 'B46', 'B47', 'B52', 'B54', 'B60', 'Q24', 'J', 'M', 'Z'],\n",
       "   (8234, 23))),\n",
       " ('12X550',\n",
       "  (['Bx11', 'Bx27', 'Bx36', 'Bx4', 'Bx4A', 'Q44', '6'], (13566, 42)))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schoolsWithLines.join(satScores).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B52', 358),\n",
       " ('M', 358),\n",
       " ('B38', 358),\n",
       " ('Q24', 358),\n",
       " ('B47', 358),\n",
       " ('B54', 358),\n",
       " ('J', 358),\n",
       " ('B46', 358),\n",
       " ('B60', 358),\n",
       " ('Z', 358)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schoolsWithLines.join(satScores) \\\n",
    "    .flatMap(lambda dbn_stat: [(line, dbn_stat[1][1]) for line in dbn_stat[1][0]]) \\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: int(x[0]/x[1])) \\\n",
    "    .sortBy(lambda x:-x[1])\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowLen = len(schools.first().split(','))\n",
    "\n",
    "def findS1115(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==rowLen:\n",
    "            (dbn, buses, subways) = (row[0], row[10], row[11])\n",
    "            if 'S1115' in buses:\n",
    "                yield buses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M15, M15-SBS, M20, M5, M9, S1115',\n",
       " 'M20, M21, M5, S1115',\n",
       " 'M15, M15-SBS, M20, M5, M9, S1115',\n",
       " 'M103, M15, M15-SBS, M20, M22, M5, M9, S1115',\n",
       " 'M20, M22, M5, M9, S1115',\n",
       " 'M103, M15, M15-SBS, M20, M22, M5, M9, S1115',\n",
       " 'M15, M15-SBS, M20, M5, M9, S1115',\n",
       " 'M20, M21, M5, S1115']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schools.mapPartitionsWithIndex(findS1115).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSchools = spark.read.load(HSD_FN, format='csv', header=True, inferSchema=True)\n",
    "dfSchools = dfSchools.na.drop(subset=['boro'])\n",
    "dfSchools = dfSchools.filter(dfSchools['total_students']>500)\n",
    "dfSchools = dfSchools.select('dbn', 'boro')\n",
    "dfSchools.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|   dbn|     boro|\n",
      "+------+---------+\n",
      "|01M450|Manhattan|\n",
      "|01M539|Manhattan|\n",
      "|01M696|Manhattan|\n",
      "|02M374|Manhattan|\n",
      "|02M400|Manhattan|\n",
      "|02M408|Manhattan|\n",
      "|02M412|Manhattan|\n",
      "|02M413|Manhattan|\n",
      "|02M416|Manhattan|\n",
      "|02M418|Manhattan|\n",
      "|02M420|Manhattan|\n",
      "|02M425|Manhattan|\n",
      "|02M475|Manhattan|\n",
      "|02M489|Manhattan|\n",
      "|02M519|Manhattan|\n",
      "|02M520|Manhattan|\n",
      "|02M529|Manhattan|\n",
      "|02M542|Manhattan|\n",
      "|02M580|Manhattan|\n",
      "|02M600|Manhattan|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSchools.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|   DBN|   sum|ntakers|\n",
      "+------+------+-------+\n",
      "|02M047|  6400|     16|\n",
      "|21K410|207575|    475|\n",
      "|30Q301| 43120|     98|\n",
      "|17K382| 22066|     59|\n",
      "|18K637| 13335|     35|\n",
      "|32K403| 18300|     50|\n",
      "|09X365| 18306|     54|\n",
      "|11X270| 22064|     56|\n",
      "|05M367| 12078|     33|\n",
      "|14K404| 24276|     68|\n",
      "|30Q575| 66420|    135|\n",
      "|13K336|  3366|      9|\n",
      "|04M635| 17712|     48|\n",
      "|24Q264| 40406|     89|\n",
      "|17K408| 19494|     57|\n",
      "|19K618| 22260|     60|\n",
      "|27Q309| 13644|     36|\n",
      "|32K552| 24388|     67|\n",
      "|13K499| 26208|     72|\n",
      "|07X600| 30400|     76|\n",
      "+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#put backtick around column names if dataset column name contains . \n",
    "dfScores = spark.read.load(SAT_FN, format='csv', header=True, inferSchema=True)\n",
    "dfScores = dfScores.select('DBN', dfScores['`SAT Math Avg. Score`'].cast('int').alias('score'),\n",
    "                          dfScores['Num of SAT Test Takers'].cast('int').alias('ntakers')).na.drop()\n",
    "dfScores1 = dfScores.select('DBN', (dfScores.score*dfScores.ntakers).alias('sum'), 'ntakers')\n",
    "#dfScores.take(5)\n",
    "dfScores1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: x[0]/x[1]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|         boro|avg|\n",
      "+-------------+---+\n",
      "|       Queens|474|\n",
      "|     Brooklyn|487|\n",
      "|Staten Island|477|\n",
      "|    Manhattan|514|\n",
      "|        Bronx|470|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfResults = dfSchools.join(dfScores1, dfSchools.dbn==dfScores1.DBN, how='inner')\n",
    "dfResults = dfResults.groupBy('boro').sum('sum', 'ntakers')\n",
    "dfResults = dfResults.withColumn('avg', (dfResults[1]/dfResults[2]).cast('int'))\n",
    "dfResults = dfResults.select('boro', 'avg')\n",
    "dfResults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
