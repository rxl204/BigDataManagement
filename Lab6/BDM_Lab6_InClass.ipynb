{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.23.52:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating two RDD, one is from the README file of Spark\n",
    "# and the other is directly from a list within the notebook.\n",
    "#\n",
    "# If you downloaded Spark, the README file is in the same folder\n",
    "# as the one you extracted. If you use other package management\n",
    "# methods like 'brew', 'dnf', 'apt', etc. you will need to figure\n",
    "# out the path of Spark by printing sys.path.\n",
    "\n",
    "rdd = sc.textFile('/Users/hvo/sw/spark/README.md')\n",
    "testRdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['#', 'Apache', 'Spark'], [], ['Spark', 'is', 'a', 'fast', 'and', 'general', 'cluster', 'computing', 'system', 'for', 'Big', 'Data.', 'It', 'provides']]\n",
      "\n",
      "['#', 'Apache', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "# Here, we're showing the difference between map() and flatMap()\n",
    "# doing the line split and get back the first 3 elements, take(3).\n",
    "# - map() is a one-to-one mapping, just like Python, so the first\n",
    "# line prints out 3 lists, each consists words per each.\n",
    "# - flatMap() is a one-to-many mapping, like MapReduce's map(). So\n",
    "# the second line prints out only 3 words.\n",
    "\n",
    "wordsPerLine = rdd.map(lambda line: line.split()).take(3)\n",
    "words = rdd.flatMap(lambda line: line.split()).take(3)\n",
    "\n",
    "print ('%s\\n\\n%s' % (wordsPerLine, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('is', 6)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the word count example with Spark using the approach\n",
    "# shown in the slides, i.e. staying true to the MapReduce paradigm.\n",
    "# Note that groupByKey() will sort and group everything together by\n",
    "# keys first. Then the function in mapValues() will each get applied\n",
    "# per each (key, list of values) pair. This could be an issue if we\n",
    "# have a pair with lots of values since all of the values have to be\n",
    "# stored in memory.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda values: sum(values))\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('is', 6)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is another approach with reduceByKey() instead of groupByKey().\n",
    "# The reduce function provided for reduceByKey() only takes 2 params\n",
    "# at a time, thus, doesn't suffer the scalability issue. It also has\n",
    "# better benefits in term of parallelism.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 25), ('to', 19), ('spark', 16)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we'd like to compute the top 3 most popular words in Spark. We\n",
    "# can use the RDD's top() function directly. This is much easier\n",
    "# than the two-step MapReduce job, where we had to first compute the\n",
    "# top 3 words per partition, then another top 3 on top of that. In\n",
    "# fact, this is exactly how Spark RDD's top() function is implemented.\n",
    "# More info can be found here:\n",
    "# https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L1249\n",
    "wc.top(3, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 6 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT_FN = 'SAT_Results.csv'\n",
    "HSD_FN = 'DOE_High_School_Directory_2014-2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a way to read CSV file from within Spark directly into a \n",
    "# Spark's DataFrame, which we will not be covering yet. Just putting\n",
    "# it here so that we have a reference for now. Note that, the \n",
    "# 'parserLib' option is important for reading multi-line fields of CSV.\n",
    "df = spark.read \\\n",
    "            .format(\"com.databricks.spark.csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"parserLib\", \"UNIVOCITY\") \\\n",
    "            .load(HSD_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'DBN'),\n",
       " (1, 'SCHOOL NAME'),\n",
       " (2, 'Num of SAT Test Takers'),\n",
       " (3, 'SAT Critical Reading Avg. Score'),\n",
       " (4, 'SAT Math Avg. Score'),\n",
       " (5, 'SAT Writing Avg. Score')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read the SAT score to our RDD. Note that the use_unicode can be\n",
    "# changed accordingly to your data file to handle Unicode. If you cannot\n",
    "# parse your data due to an 'utf8' or 'ascii' decoding issue, it might\n",
    "# be a good thing to try flipping the use_unicode parameter here.\n",
    "\n",
    "sat = sc.textFile(SAT_FN, use_unicode=True).cache()\n",
    "\n",
    "# This line for us to list the column index and column names to see\n",
    "# which column we need to use for our task. In this case, we're\n",
    "# interested in the number of test takers (#2) and the math score (#4).\n",
    "list(enumerate(sat.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN,SCHOOL NAME,Num of SAT Test Takers,SAT Critical Reading Avg. Score,SAT Math Avg. Score,SAT Writing Avg. Score\n",
      "02M047,47 THE AMERICAN SIGN LANGUAGE AND ENGLISH SECONDARY SCHOOL,16,395,400,387\n"
     ]
    }
   ],
   "source": [
    "# Note that, our data input includes a header line that we don't want to\n",
    "# use in analysis. We can remove the header line from our RDD by doing\n",
    "# a 'filter' to remove all rows that matches the header like below. Though\n",
    "# this works, it means that we have to apply the filter function on *all*\n",
    "# row, which could be a lot of computation.\n",
    "\n",
    "noHeaderRDD = sat.filter(lambda x: not x.startswith('DBN,SCHOOL'))\n",
    "print (sat.first())\n",
    "print (noHeaderRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('02M047', (6400, 16)),\n",
       " ('21K410', (207575, 475)),\n",
       " ('30Q301', (43120, 98)),\n",
       " ('17K382', (22066, 59)),\n",
       " ('18K637', (13335, 35))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can perform the header checking per-partition, instead\n",
    "# of per-row like below. mapPartitions() is another type of map operators\n",
    "# in Spark that is similar to Hadoop Streaming's map(). It is many-to-many.\n",
    "# RDD in Spark are divided into partitions (as we read or as provided by\n",
    "# HDFS), each partition can be processed in parallel using a function\n",
    "# supplied to the mapPartitions() call.\n",
    "# \n",
    "# In addition to mapPartitions(), Spark also provides a variation called\n",
    "# mapPartitionsWithIndex() that provides information on which partition\n",
    "# we are currently processing. Indeed, mapPartitionsWithIndex() is the\n",
    "# the operator with the lowest overhead (since mapPartitions() get mapped\n",
    "# to mapPartitionsWithIndex) and also the most efficient one among all the\n",
    "# map operators.\n",
    "#\n",
    "# So our logic below is to use the partition index to check if we're hitting\n",
    "# the header (aka the first partition). If so, we just skip the first row.\n",
    "\n",
    "def extractScores(partId, records):\n",
    "    if partId==0:\n",
    "        next(records)\n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for row in reader:\n",
    "        if row[2]!='s': # to filter our bad-quality data\n",
    "            (dbn,takers,score) = (row[0], int(row[2]), int(row[4]))\n",
    "            yield (dbn, (score*takers, takers))\n",
    "\n",
    "satScores = sat.mapPartitionsWithIndex(extractScores)\n",
    "satScores.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'dbn'),\n",
       " (1, 'school_name'),\n",
       " (2, 'boro'),\n",
       " (3, 'building_code'),\n",
       " (4, 'phone_number'),\n",
       " (5, 'fax_number'),\n",
       " (6, 'grade_span_min'),\n",
       " (7, 'grade_span_max'),\n",
       " (8, 'expgrade_span_min'),\n",
       " (9, 'expgrade_span_max'),\n",
       " (10, 'bus'),\n",
       " (11, 'subway'),\n",
       " (12, 'primary_address_line_1'),\n",
       " (13, 'city'),\n",
       " (14, 'state_code'),\n",
       " (15, 'zip'),\n",
       " (16, 'website'),\n",
       " (17, 'total_students'),\n",
       " (18, 'campus_name'),\n",
       " (19, 'school_type'),\n",
       " (20, 'overview_paragraph'),\n",
       " (21, 'program_highlights'),\n",
       " (22, 'language_classes'),\n",
       " (23, 'advancedplacement_courses'),\n",
       " (24, 'online_ap_courses'),\n",
       " (25, 'online_language_courses'),\n",
       " (26, 'extracurricular_activities'),\n",
       " (27, 'psal_sports_boys'),\n",
       " (28, 'psal_sports_girls'),\n",
       " (29, 'psal_sports_coed'),\n",
       " (30, 'school_sports'),\n",
       " (31, 'partner_cbo'),\n",
       " (32, 'partner_hospital'),\n",
       " (33, 'partner_highered'),\n",
       " (34, 'partner_cultural'),\n",
       " (35, 'partner_nonprofit'),\n",
       " (36, 'partner_corporate'),\n",
       " (37, 'partner_financial'),\n",
       " (38, 'partner_other'),\n",
       " (39, 'addtl_info1'),\n",
       " (40, 'addtl_info2'),\n",
       " (41, 'start_time'),\n",
       " (42, 'end_time'),\n",
       " (43, 'se_services'),\n",
       " (44, 'ell_programs'),\n",
       " (45, 'school_accessibility_description'),\n",
       " (46, 'number_programs'),\n",
       " (47, 'priority01'),\n",
       " (48, 'priority02'),\n",
       " (49, 'priority03'),\n",
       " (50, 'priority04'),\n",
       " (51, 'priority05'),\n",
       " (52, 'priority06'),\n",
       " (53, 'priority07'),\n",
       " (54, 'priority08'),\n",
       " (55, 'priority09'),\n",
       " (56, 'priority10'),\n",
       " (57, 'Location 1')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the same thing with the school directory data\n",
    "schools = sc.textFile(HSD_FN, use_unicode=True).cache()\n",
    "list(enumerate(schools.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSchools(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58 and row[17].isdigit():\n",
    "            (dbn, boro, total_students) = (row[0], row[2], int(row[17]))\n",
    "            if total_students>500: # filter to keep the large schools\n",
    "                yield (dbn, boro)\n",
    "\n",
    "largeSchools = schools.mapPartitionsWithIndex(extractSchools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: int(x[0]/x[1])) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bronx', 470),\n",
       " ('Staten Island', 477),\n",
       " ('Manhattan', 514),\n",
       " ('Brooklyn', 487),\n",
       " ('Queens', 474)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dbn,school_name,boro,building_code,phone_number,fax_number,grade_span_min,grade_span_max,expgrade_span_min,expgrade_span_max,bus,subway,primary_address_line_1,city,state_code,zip,website,total_students,campus_name,school_type,overview_paragraph,program_highlights,language_classes,advancedplacement_courses,online_ap_courses,online_language_courses,extracurricular_activities,psal_sports_boys,psal_sports_girls,psal_sports_coed,school_sports,partner_cbo,partner_hospital,partner_highered,partner_cultural,partner_nonprofit,partner_corporate,partner_financial,partner_other,addtl_info1,addtl_info2,start_time,end_time,se_services,ell_programs,school_accessibility_description,number_programs,priority01,priority02,priority03,priority04,priority05,priority06,priority07,priority08,priority09,priority10,Location 1',\n",
       " '01M292,Henry Street School for International Studies,Manhattan,M056,212-406-9411,212-406-9417,6,12,,,\"B39, M14A, M14D, M15, M15-SBS, M21, M22, M9\",\"B, D to Grand St ; F to East Broadway ; J, M, Z to Delancey St-Essex St\",220 Henry Street,New York,NY,10002,http://schools.nyc.gov/schoolportals/01/M292,323,N/A,,\"Henry Street School for International Studies is a unique small school founded by the Asia Society. While in pursuit of knowledge about other world regions, including their histories, economies and world languages, students acquire the knowledge and skills needed to prepare for college and/or careers. Teachers and other adults who make up the learning community forge supportive relationships with students and parents while providing challenging and engaging learning experiences. Our school partners with various community, arts and business organizations to help students achieve success. Our theme of international studies extends beyond the classroom, where students participate in ongoing ‘Advisory Day Out’ excursions where the multiculturalism of NYC becomes the classroom.\",\"Global/International Studies in core subjects, Literacy block schedule, Personalized instruction in small classes, Student Advisories, International travel opportunities, After-school program focused on youth leadership\",\"Chinese (Mandarin), Spanish\",Psychology,\"Chinese Language and Culture, Spanish Literature and Culture\",\"Chinese (Mandarin), Spanish\",\"Math through Card Play; Art, Poetry/Spoken Word, Drama, Book, STEP, Big Brothers/Big Sisters, Student Government/Leadership, Future Project\",Basketball,Softball,Soccer,\"Boxing, Track, CHAMPS, Tennis, Flag Football, Softball\",The Henry Street Settlement; Asia Society; America Reads; Future Project; 21st Century Grant,Gouverneur Hospital (Turning Points),New York University,Asia Society,Heart of America Foundation,,,United Nations,,,8:30 AM,3:30 PM,This school will provide students with disabilities the supports and services indicated on their IEPs.,ESL,Functionally Accessible,1,Priority to continuing 8th graders,Then to Manhattan students or residents who attend an information session,Then to New York City residents who attend an information session,Then to Manhattan students or residents,Then to New York City residents,,,,,,\"220 Henry Street',\n",
       " 'New York, NY 10002']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schools.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "buses = \"B39, M14A, M14D, M15, M15-SBS, M21, M22, M9\"\n",
    "subways = \"B, D to Grand St ; F to East Broadway ; J, M, Z to Delancey St-Essex St\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'D', 'F', 'J', 'M', 'Z']\n"
     ]
    }
   ],
   "source": [
    "lines = buses.split(', ')\n",
    "lines = ', '.join([direction.split(' to ')[0] \n",
    "                   for direction in subways.split(' ; ')]).split(', ')\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowLen = len(schools.first().split(','))\n",
    "\n",
    "def extractSchools2(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==rowLen:\n",
    "            (dbn, buses, subways) = (row[0], row[10], row[11])\n",
    "            lines = buses.split(', ') + ', '.join([direction.split(' to ')[0] \n",
    "                   for direction in subways.split(' ; ')]).split(', ')\n",
    "            yield (dbn, lines)\n",
    "\n",
    "schoolsWithLines = schools.mapPartitionsWithIndex(extractSchools2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT:\n",
    "(school, [line]) x (school, score) -> [(school, ([line], score))]           # JOIN\n",
    "                                   -> [([line], score)]                        # EXTRACT VALUES\n",
    "                                   -> [(line1, score), (line2, score), ...]    # REPLICATE\n",
    "                                    -> (line, [score])                         # REDUCE\n",
    "[['A', 'B', 'C'], (x, y)] -> ('A', (x,y)), ('B', (x,y)), ('C', (x,y))\n",
    "OUTPUT:\n",
    "(line, avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S1115', 612),\n",
       " ('M79', 594),\n",
       " ('Q42', 582),\n",
       " ('M22', 574),\n",
       " ('Bx3', 571),\n",
       " ('B52', 560),\n",
       " ('B63', 557),\n",
       " ('B69', 548),\n",
       " ('B54', 543),\n",
       " ('B25', 541)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schoolsWithLines.join(satScores) \\\n",
    "    .flatMap(lambda dbn_stats: [(line, dbn_stats[1][1]) for line in dbn_stats[1][0]]) \\\n",
    "    .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: int(x[0]/x[1])) \\\n",
    "    .sortBy(lambda x: -x[1]) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSchools = spark.read.load(HSD_FN, format='csv', header=True, inferSchema=True)\n",
    "dfSchools = dfSchools.na.drop(subset=['boro'])\n",
    "dfSchools = dfSchools.filter(dfSchools['total_students']>500)\n",
    "dfSchools = dfSchools.select('dbn', 'boro')\n",
    "dfSchools.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DBN='02M047', sum=6400, ntakers=16)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfScores = spark.read.load(SAT_FN, format='csv', header=True, inferSchema=True)\n",
    "dfScores = dfScores.select('DBN', \n",
    "                           dfScores['`SAT Math Avg. Score`'].cast('int').alias('score'), \n",
    "                           dfScores['Num of SAT Test Takers'].cast('int').alias('ntakers')\n",
    "                          ).na.drop()\n",
    "dfScores1 = dfScores.select('DBN',\n",
    "                            (dfScores.score*dfScores.ntakers).alias('sum'),\n",
    "                            'ntakers')\n",
    "dfScores1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: int(x[0]/x[1])) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|         boro|avg|\n",
      "+-------------+---+\n",
      "|       Queens|474|\n",
      "|     Brooklyn|487|\n",
      "|Staten Island|477|\n",
      "|    Manhattan|514|\n",
      "|        Bronx|470|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfResults = dfSchools.join(dfScores1, dfSchools.dbn==dfScores1.DBN, how='inner')\n",
    "dfResults = dfResults.groupBy('boro').sum('sum', 'ntakers')\n",
    "dfResults = dfResults.withColumn('avg', (dfResults[1]/dfResults[2]).cast('int'))\n",
    "dfResults = dfResults.select('boro', 'avg')\n",
    "dfResults.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis",
   "language": "python",
   "name": "vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
